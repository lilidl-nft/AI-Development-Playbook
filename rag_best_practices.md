# ü§ñ RAG Best Practices (Retrieval-Augmented Generation)

## üß† Core Principles
*Quality context leads to quality answers. The LLM is only as good as the data you feed it.*

1.  **Garbage In, Garbage Out**: If your source documents are messy, your retrieval will fail. Clean data is paramount.
2.  **Context is King**: The goal is to provide the *exact* relevant snippet, not the whole document.
3.  **Latency vs. Accuracy**: Balance the cost of re-ranking and complex chains with the user's need for speed.
4.  **Traceability**: Always know *why* an answer was generated by citing the source.

## üìÑ Data Ingestion & Chunking
*How you break down data determines what you can find later.*

-   **Cleaning**: Remove headers, footers, HTML tags, and duplicate whitespace before chunking.
-   **Chunking Strategy**:
    -   **Fixed-size**: Simple, but risks cutting context (e.g., 512 tokens).
    -   **Semantic**: Break by paragraphs, markdown headers, or logical sections. **Preferred**.
    -   **Overlap**: Always include overlap (e.g., 10-20%) to preserve context between chunks.
-   **Metadata**: Enrich chunks with metadata (source URL, author, date, category) to enable **filtering** later.

## üß¨ Embeddings & Vector Storage

-   **Model Selection**: Choose an embedding model optimized for your language and domain (e.g., OpenAI `text-embedding-3`, or open-source `bge-m3` via `sentence-transformers`).
-   **Distance Metric**: Use **Cosine Similarity** for text embeddings.
-   **Vector DB**:
    -   Use a dedicated store (Qdrant, Chroma, Pinecone) or `pgvector` (PostgreSQL) if you want to keep the stack simple.
    -   **Indexing**: Ensure HNSW indexes are built for production performance.

## üîç Retrieval & Ranking (The "R" in RAG)

-   **Hybrid Search**: Combine **Sparse Vectors** (BM25/Keyword search) with **Dense Vectors** (Semantic search).
    -   *Why?* Semantic search misses specific acronyms or part numbers; keywords catch them.
-   **Re-Ranking**: Use a Cross-Encoder (e.g., Cohere Rerank or BGE-Reranker) to re-score the top K results.
    -   *Flow*: Retrieve top 50 -> Re-rank -> Pass top 5 to LLM.
-   **Query Expansion**: Reword the user's query or generate hypothetical answers (HyDE) to improve retrieval match.

## üí¨ Generation (The "G" in RAG)

-   **System Prompt**: Explicitly instruct the LLM to use *only* the provided context.
    -   *Pattern*: "Answer the question based strictly on the context below. If the answer is not in the context, say 'I don't know'."
-   **Citations**: Instruct the LLM to return source IDs or page numbers with every claim.
-   **Context Window**: Don't stuff the context window. Use the re-ranked top results to reduce noise and cost.

## üìè Evaluation & Observability
*Don't guess; measure.*

-   **Frameworks**: Use **RAGAS** or **DeepEval** to automate testing.
-   **Key Metrics**:
    -   **Faithfulness**: Is the answer derived from the context?
    -   **Answer Relevance**: Does the answer address the query?
    -   **Context Precision**: Was the relevant chunk actually retrieved?
    -   **Context Recall**: Is the answer present in the retrieved chunks?
-   **Feedback Loop**: Implement a "Thumbs Up/Down" in the UI to capture user feedback on answers.
